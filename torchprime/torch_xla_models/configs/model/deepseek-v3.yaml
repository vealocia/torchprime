defaults:
  - _self_  # refers to this config file
  - sharding: deepseek-fsdp-tp-ep  # refers to sharding/deepseek-fsdp-tp.yaml
  - remat: deepseek  # refers to remat/deepseek.yaml

model_id: deepseek-v3
model_class: deepseek_v3.DeepseekV3ForCausalLM
tokenizer_name: deepseek-ai/deepseek-v3
# choose attention_kernel from: [splash_attention, null] # flash_attention does not work with DeepSeek V3
attention_kernel: splash_attention
capacity_factor: 1.25

# Configuration automatically generated from HF 
vocab_size: 129280
max_position_embeddings: 4096
hidden_size: 7168
intermediate_size: 18432
moe_intermediate_size: 2048
num_hidden_layers: 61
num_attention_heads: 128
n_shared_experts: 1
n_routed_experts: 256
routed_scaling_factor: 2.5
kv_lora_rank: 512
q_lora_rank: 1536
qk_rope_head_dim: 64
v_head_dim: 128
qk_nope_head_dim: 128
qk_head_dim: 192
head_dim: 64
n_group: 8
topk_group: 4
num_experts_per_tok: 8
first_k_dense_replace: 3
norm_topk_prob: true
rope_interleave: true
rope_scaling:
  beta_fast: 32
  beta_slow: 1
  factor: 40
  mscale: 1.0
  mscale_all_dim: 1.0
  original_max_position_embeddings: 4096
  rope_type: "yarn"
  type: "yarn"
num_key_value_heads: 128
hidden_act: silu
initializer_range: 0.02
rms_norm_eps: 1.0e-06
rope_theta: 10000
attention_bias: false
attention_dropout: 0.0
return_dict: true
output_hidden_states: false
output_attentions: false
torchscript: false
torch_dtype: bfloat16
use_bfloat16: false
bos_token_id: 0
pad_token_id: null
eos_token_id: 1