# 2D (FSDP + TP) sharding configuration for Llama4 models.

# Weights

# vocab_size, hidden_size
model.embed_tokens.weight: [fsdp, tensor]
# hidden_size, num_attention_heads * head_dim
model.layers.*.self_attn.q_proj.weight: [tensor, fsdp]
# hidden_size, num_key_value_heads * head_dim
model.layers.*.self_attn.k_proj.weight: [tensor, fsdp]
# hidden_size, num_key_value_heads * head_dim
model.layers.*.self_attn.v_proj.weight: [tensor, fsdp]
# hidden_size, hidden_size
model.layers.*.self_attn.o_proj.weight: [fsdp, tensor]

# num_experts, hidden_size, 2 * intermediate_size
model.layers.*.feed_forward.experts.gate_up_proj: [null, tensor, fsdp]  
# num_experts, expert_dim, hidden_size
model.layers.*.feed_forward.experts.down_proj:  [null, fsdp, tensor]
# hidden_size, num_experts
model.layers.*.feed_forward.router.weight: [fsdp, null]
# hidden_size, intermediate_size    
model.layers.*.feed_forward.shared_expert.gate_proj.weight: [tensor, fsdp]
# hidden_size, intermediate_size    
model.layers.*.feed_forward.shared_expert.up_proj.weight: [tensor, fsdp]
# intermediate_size, hidden_size
model.layers.*.feed_forward.shared_expert.down_proj.weight: [fsdp, tensor]
# hidden_size
model.layers.*.input_layernorm.weight: [fsdp]
# hidden_size
model.layers.*.post_attention_layernorm.weight: [fsdp]
# hidden_size
model.norm.weight: [fsdp]
# hidden_size, vocab_size
lm_head.weight: [tensor, fsdp]

# Activations
model.layers.*: [[data, fsdp], null, tensor]
lm_head: [[data, fsdp], null, tensor]
