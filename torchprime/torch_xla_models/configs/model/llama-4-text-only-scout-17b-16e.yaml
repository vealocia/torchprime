# Modelled by
# https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E/blob/main/config.json
defaults:
  - _self_  # refers to this config file
  - sharding: llama4-fsdp-tp  # refers to sharding/llama4-fsdp-tp.yaml
  - remat: llama4  # refers to remat/llama4.yaml

model_class: llama4.Llama4TextForCausalLM  # Used to import the model from this class
tokenizer_name: meta-llama/Llama-4-Scout-17B-16E
torch_dtype: bfloat16
use_bfloat16: true
temperature: 1.0
bos_token_id: 200000
pad_token_id: 200018
eos_token_id:
- 200001
- 200007
- 200008
attention_bias: false
model_type: llama4_text
attn_temperature_tuning: 4  # not sure if needed
attn_scale: 0.1
floor_scale: 8192
vocab_size: 202048
max_position_embeddings: 262144
hidden_size: 5120
intermediate_size: 8192
intermediate_size_mlp: 16384
num_hidden_layers: 48
num_attention_heads: 40
rope_scaling:
  factor: 16.0
  high_freq_factor: 1.0
  low_freq_factor: 1.0
cache_implementation: hybrid # not sure if needed
num_key_value_heads: 8
hidden_act: silu
initializer_range: 0.02
rms_norm_eps: 1.0e-05
use_cache: true      # not sure if needed
rope_theta: 500000.0
attention_dropout: 0.0
head_dim: 128
use_qk_norm: true
num_experts_per_tok: 1
num_local_experts: 16
output_router_logits: false
router_aux_loss_coef: 0.001
router_jitter_noise: 0.0
no_rope_layers:
- 1
- 1
- 1
- 0
- 1
- 1
- 1
- 0
- 1
- 1
- 1
- 0
- 1
- 1
- 1
- 0
- 1
- 1
- 1
- 0
- 1
- 1
- 1
- 0
- 1
- 1
- 1
- 0
- 1
- 1
- 1
- 0
- 1
- 1
- 1
- 0
- 1
- 1
- 1
- 0
- 1
- 1
- 1
- 0
- 1
- 1
- 1
- 0
moe_layers:
- 0
- 1
- 2
- 3
- 4
- 5
- 6
- 7
- 8
- 9
- 10
- 11
- 12
- 13
- 14
- 15
- 16
- 17
- 18
- 19
- 20
- 21
- 22
- 23
- 24
- 25
- 26
- 27
- 28
- 29
- 30
- 31
- 32
- 33
- 34
- 35
- 36
- 37
- 38
- 39
- 40
- 41
- 42
- 43
- 44
- 45
- 46
- 47
attention_chunk_size: 8192
attention_kernel: flash_attention
moe_implementation: expert_parallelism
