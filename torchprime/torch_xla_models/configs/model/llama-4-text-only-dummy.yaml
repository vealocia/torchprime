# Modelled by a scalled down version of
# https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E/blob/main/config.json
defaults:
  - _self_  # refers to this config file
  - sharding: llama4-fsdp  # refers to sharding/llama4-fsdp.yaml
  - remat: llama4  # refers to remat/llama4.yaml

model_id: llama-4-text-only-dummy
model_class: llama4.Llama4TextForCausalLM  # Used to import the model from this class
tokenizer_name: meta-llama/Llama-4-Scout-17B-16E
use_bfloat16: true
temperature: 1.0
bos_token_id: 200000
pad_token_id: 127
eos_token_id:
- 200001
- 200007
- 200008
attention_bias: false
model_type: llama4_text
attn_scale: 0.1
floor_scale: 8192
vocab_size: 1280
max_position_embeddings: 262144
hidden_size: 512
intermediate_size: 64
intermediate_size_mlp: 16384
num_hidden_layers: 2
num_attention_heads: 8
rope_scaling:
  factor: 16.0
  high_freq_factor: 1.0
  low_freq_factor: 1.0
num_key_value_heads: 8
hidden_act: silu
initializer_range: 0.02
rms_norm_eps: 1.0e-05
rope_theta: 500000.0
attn_temperature_tuning: true
attention_dropout: 0.0
head_dim: 64
use_qk_norm: true
num_experts_per_tok: 1
num_local_experts: 16
output_router_logits: false
router_aux_loss_coef: 0.001
router_jitter_noise: 0.0
no_rope_layers:
- 1
- 1
interleave_moe_layer_step: 1
moe_layers:
- 0
- 1
attention_chunk_size: 8192
attention_kernel: null
moe_implementation: sequential
