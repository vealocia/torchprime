benchmarks:
  llama-3-8b:
    name: Llama 3.0 8B
    step_time_lower_bound: 1.493623
    step_time_upper_bound: 3.35579591
    confidence_interval: 0.93109
    average: 2.4247
    sample_size: 156
  llama-3_1-8b-sa:
    name: Llama 3.1 8B (Splash Attention)
    step_time_lower_bound: 2.36514293
    step_time_upper_bound: 2.458194
    confidence_interval: 0.04653
    average: 2.4117
    sample_size: 99
  llama-3_1-8b-scan-offload:
    name: Llama 3.1 8B (Scan + Offload)
    step_time_lower_bound: 2.76316427
    step_time_upper_bound: 2.84732156
    confidence_interval: 0.04208
    average: 2.8052
    sample_size: 102
  llama-3-8b-2d:
    name: Llama 3.0 8B (2D sharding)
    step_time_lower_bound: 3.31089736
    step_time_upper_bound: 3.41173688
    confidence_interval: 0.05042
    average: 3.3613
    sample_size: 117
  mixtral-8x7b:
    name: Mixtral 8x7B
    step_time_lower_bound: 3.11970059
    step_time_upper_bound: 3.21471685
    confidence_interval: 0.04751
    average: 3.1672
    sample_size: 110
  llama-3-8b-2-slice:
    name: Llama 3.0 8B (2 Slice)
    step_time_lower_bound: 3.91503271
    step_time_upper_bound: 4.04127
    confidence_interval: 0.06312
    average: 3.9782
    sample_size: 115
  llama-3-8b-sft:
    name: Llama 3.0 8B SFT
    step_time_lower_bound: 0.82997861
    step_time_upper_bound: 1.101424
    confidence_interval: 0.13572
    average: 0.9657
    sample_size: 114
    target_loss: 0.4735
    loss_tolerance: 0.001
  llama-3-8b-ddp-fsdp:
    name: Llama 3.0 8B (ddp + fsdp)
    step_time_lower_bound: 3.22628088
    step_time_upper_bound: 3.346441
    confidence_interval: 0.06008
    average: 3.2864
    sample_size: 113
  llama-3-8b-fsdp-cp:
    name: Llama 3.0 8B (fsdp + cp)
    step_time_lower_bound: 1.49
    step_time_upper_bound: 1.64146789
    confidence_interval: 0.02426
    average: 1.6172
    sample_size: 51
  ds-v3-shallow:
    name: Deepseek v3 Shallow # dummy number
    step_time_lower_bound: 0.1
    step_time_upper_bound: 1.1
    confidence_interval: 0.5
    average: 0.6
    sample_size: 10
  llama-3-8b-pure-mlp:
    name: Llama 3.0 8B (@assume_pure) # dummy number
    step_time_lower_bound: 1.5
    step_time_upper_bound: 3.4
    confidence_interval: 0.9
    average: 2.45
    sample_size: 150
metadata:
  query_start: '2025-07-01T00:00:00-07:00'
  query_end: '2025-07-23T12:20:48-07:00'
  confidence_level: 0.999
